{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finalPreprocessingCode.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOzQvwYC/PbL9mkjNmHYiEO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MalshaPiumini/FYP2022/blob/test-01/finalPreprocessingCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **I M P O R T S**"
      ],
      "metadata": {
        "id": "o4artSIot9O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode\n",
        "!pip install emoji --upgrade"
      ],
      "metadata": {
        "id": "wn87xcJDE_hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYPoMeFqyHHK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from unidecode import unidecode\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import psycopg2 as ps\n",
        "import string\n",
        "import emoji\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(emoji.emojize('Python es :pulgar_hacia_arriba:', language='es'))"
      ],
      "metadata": {
        "id": "r3wtesJc9WvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **O P E N__D A T A S E T ✌**"
      ],
      "metadata": {
        "id": "O0MnTcA9CzEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gif_IzZAI7Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hateSpeechData = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/FYP Implementations/March-test2.csv\" )\n",
        "\n",
        "#Setting the column width\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "#Setting the column names\n",
        "hateSpeechData.columns = ['Text','Feeling', 'Emotion','Label']\n",
        "\n",
        "#Printing first 5 rows\n",
        "hateSpeechData.head()"
      ],
      "metadata": {
        "id": "EXVnDKYL2X35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the shape of training and testing dataset\n",
        "print(hateSpeechData.shape)"
      ],
      "metadata": {
        "id": "csSdHJZv5j_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check null or NaN values are present in the training data and testing data\n",
        "print(hateSpeechData.isnull().values.any())"
      ],
      "metadata": {
        "id": "3Gkx0refCOer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape of the dataset\n",
        "print(\"Input data has {} rows and {} columns\".format(len(hateSpeechData), len(hateSpeechData.columns)))"
      ],
      "metadata": {
        "id": "X-5mqLgxk_6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many cb/non cb texts are there?\n",
        "print(\"Out of {} rows, {} are Cyberbullying, {} are Not-cyberbullying\".format(len(hateSpeechData),\n",
        "                                                       len(hateSpeechData[hateSpeechData['Label']==1]),\n",
        "                                                       len(hateSpeechData[hateSpeechData['Label']==0])))"
      ],
      "metadata": {
        "id": "jzo-VCDDlGPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting all the labels to upper case\n",
        "hateSpeechData['Feeling'] = hateSpeechData['Feeling'].str.lower()\n",
        "hateSpeechData['Emotion'] = hateSpeechData['Emotion'].str.lower()\n",
        "hateSpeechData.head()"
      ],
      "metadata": {
        "id": "cbdXnhqIl4Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feelings in the dataset\n",
        "print(\"Out of {} rows, {} are Anger, {} are Suprise, {} are Happy, {} are Sad, {} are Disgust, {} are Neutral\".format(len(hateSpeechData),\n",
        "                                                       len(hateSpeechData[hateSpeechData['Feeling']=='anger']),\n",
        "                                                       len(hateSpeechData[hateSpeechData['Feeling']=='suprise']),\n",
        "                                                       len(hateSpeechData[hateSpeechData['Feeling']=='happy']),\n",
        "                                                       len(hateSpeechData[hateSpeechData['Feeling']=='sad']),\n",
        "                                                       len(hateSpeechData[hateSpeechData['Feeling']=='disgust']),\n",
        "                                                       len(hateSpeechData[hateSpeechData['Feeling']=='neutral'])))"
      ],
      "metadata": {
        "id": "xvkcUnrHl5YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Emotions in the dataset\n",
        "print(\"Out of {} rows, {} are Anger, {} are Suprise, {} are Happy, {} are Sad,{} are Disgust, {} are Neutral\".format(len(hateSpeechData),\n",
        "                                                       len(hateSpeechData[hateSpeechData['Emotion']=='anger']),\n",
        "                                                       len(hateSpeechData[hateSpeechData['Emotion']=='suprise']),\n",
        "                                                       len(hateSpeechData[hateSpeechData['Emotion']=='happy']),\n",
        "                                                       len(hateSpeechData[hateSpeechData['Emotion']=='sad']),\n",
        "                                                       len(hateSpeechData[hateSpeechData['Emotion']=='disgust']),\n",
        "                                                       len(hateSpeechData[hateSpeechData['Emotion']=='neutral'])))"
      ],
      "metadata": {
        "id": "wH1MX_R5mMS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the dataset row by row\n",
        "from csv import reader\n",
        "# skip first line i.e. read header first and then iterate over each row od csv as a list\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/FYP Implementations/March-test2.csv', 'r') as read_obj:\n",
        "    csv_reader = reader(read_obj)\n",
        "    header = next(csv_reader)\n",
        "    if header != None:\n",
        "        for row in csv_reader:\n",
        "            print(row)"
      ],
      "metadata": {
        "id": "EU-B2KJG_AFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **C L E A N I N G ✌**"
      ],
      "metadata": {
        "id": "FenJqxp6ArSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import emoji as emj\n",
        "\n",
        "EMOJIS = emj.UNICODE_EMOJI[\"en\"]\n",
        "\n",
        "#Move emojies to another column\n",
        "def add_emoji_column(df):\n",
        "  emoji_column = \"\"\n",
        "  for emoji in EMOJIS:\n",
        "    if emoji in df:\n",
        "      emoji_column = emoji_column + emoji\n",
        "  return emoji_column"
      ],
      "metadata": {
        "id": "V6CJ4o79Af8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hateSpeechData['emoji']=hateSpeechData['Text'].apply(lambda x : add_emoji_column(x))\n",
        "hateSpeechData.head()"
      ],
      "metadata": {
        "id": "Q2j6Mwv8skqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Emojies from the text\n",
        "def remove_emojies(df):\n",
        "  cleaned_emojies = \"\"\n",
        "  for emoji in EMOJIS:\n",
        "    if emoji in df:\n",
        "      df = df.replace(emoji, \" \")\n",
        "  return df"
      ],
      "metadata": {
        "id": "t2XLIa_MAf1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hateSpeechData['emoji_cleaned_text']=hateSpeechData['Text'].apply(lambda x : remove_emojies(x))\n",
        "hateSpeechData.head() "
      ],
      "metadata": {
        "id": "dtZFTySbs0Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sinhala Unicode -> Romanized Sinhala\n",
        "def sinhalaToSinglish(inputDf):\n",
        "  for index in range(len(inputDf)):\n",
        "      currTokenList = inputDf['emoji_cleaned_text'].values[index]\n",
        "      singlishTokenList = []\n",
        "      for token in currTokenList:\n",
        "        singlishTokenList.append(unidecode(token))\n",
        "        inputDf['emoji_cleaned_text'].values[index] = \"\".join(singlishTokenList);\n",
        "  return inputDf"
      ],
      "metadata": {
        "id": "Jpwxs53wAfuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sinhalaToSinglish(hateSpeechData)\n",
        "hateSpeechData.head()"
      ],
      "metadata": {
        "id": "CtA6_fEps8E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hateSpeechData['emoji_cleaned_text'] = hateSpeechData['emoji_cleaned_text'].str.lower()"
      ],
      "metadata": {
        "id": "LRA_w2WHKhEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove URLs\n",
        "def urlRemoving(inputDf):\n",
        "  for i in range(len(inputDf)):\n",
        "    currentPhase= inputDf['emoji_cleaned_text'].values[i]\n",
        "    inputDf['emoji_cleaned_text'].values[i] = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', currentPhase)\n",
        "  return inputDf "
      ],
      "metadata": {
        "id": "kZQ3Ck0fAflz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Dataset\n",
        "urlRemoving(hateSpeechData)\n",
        "hateSpeechData.head()"
      ],
      "metadata": {
        "id": "O31g2pY5tMm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean training data and testing data\n",
        "# remove html tags\n",
        "def remove_tags(string):\n",
        "    result = re.sub('<.*?>','',string)\n",
        "    return result\n",
        "\n",
        "# remove line seperators\n",
        "def remove_line_seperators(string):\n",
        "    result=re.sub(\"\\\\n\", \"\", string)\n",
        "    return result\n",
        "\n",
        "# remove repeated characters\n",
        "def remove_repeated_characters(text):\n",
        "    regex_pattern = re.compile(r'(.)\\1+')\n",
        "    clean_text = regex_pattern.sub(r'\\1\\1', text)\n",
        "    return clean_text\n",
        "\n",
        "# # seperate digit with text\n",
        "# def separate_digit_text(text):\n",
        "#     regex_patter = re.compile(r'([\\d]+)([a-zA-Z]+)')\n",
        "#     clean_text = regex_patter.sub(r'\\1 \\2', text)\n",
        "#     return clean_text\n",
        "\n",
        "# remove punctuations\n",
        "def remove_punctuations(text):\n",
        "    test_punc_removed = [(ch if ch not in string.punctuation else \" \") for ch in text ]\n",
        "    test_punc_removed_join = ''.join(test_punc_removed)\n",
        "    return test_punc_removed_join\n",
        "\n",
        "# remove extra space\n",
        "def remove_extra_space(text):\n",
        "    clean_text = ' '.join(text.strip().split())\n",
        "    return clean_text\n",
        "\n",
        "# clean all text\n",
        "def cleantext(text):\n",
        "    text_cleaned= remove_line_seperators(\n",
        "            remove_extra_space(\n",
        "                remove_punctuations(\n",
        "                    remove_repeated_characters(remove_tags(text)            \n",
        "                            )\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "    # text_cleaned=separate_digit_text(text_cleaned)\n",
        "    return text_cleaned"
      ],
      "metadata": {
        "id": "54-oYeQAAkb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean training data\n",
        "hateSpeechData['comments_fixed']=hateSpeechData['emoji_cleaned_text'].apply(lambda cw : cleantext(cw))\n",
        "hateSpeechData.head()"
      ],
      "metadata": {
        "id": "ggiYiF4WAlNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **T O K E N I Z E**"
      ],
      "metadata": {
        "id": "j_xjYqSpPyOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    tokens = re.split('\\W+', text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "P3Xahd0dLixQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize training data\n",
        "trainingData['body_text_tokenized'] = trainingData['comments_fixed'].apply(lambda x: tokenize(x.lower()))\n",
        "trainingData.head()"
      ],
      "metadata": {
        "id": "fw5du-SCL090"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize testing data\n",
        "testingData['body_text_tokenized'] = testingData['comments_fixed'].apply(lambda x: tokenize(x.lower()))\n",
        "testingData.head()"
      ],
      "metadata": {
        "id": "vuKoy1SvMfgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **S T O P__W O R D__R E M O V A L**"
      ],
      "metadata": {
        "id": "B2VUuHZBPso3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to count the total tokens\n",
        "def getTotalTokens(token_set):\n",
        "    count = 0\n",
        "    for tSet in token_set:\n",
        "        count += len(tSet)\n",
        "    return count"
      ],
      "metadata": {
        "id": "LJgPLSmjvMFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to count the total unique tokens\n",
        "def getTotalUniqueTokens(token_set):\n",
        "    count = 0\n",
        "    for tSet in token_set:\n",
        "        uniqueTokens = set(tSet)\n",
        "        count += len(uniqueTokens)\n",
        "    return count"
      ],
      "metadata": {
        "id": "HpRPSot-KwKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------COMPARE REMOVING AND WITHOUT REMOVING----------------------------\n",
        "#Remove Stop words\n",
        "stopword = ['dha', 'ya', 'mea', 'ea', 'haa', 'ma', 'bawa', 'nam', 'dhe', 'shdhaha', 'yi',\n",
        "                                'shaha', 'lesa', 'hoo', 'aetha', 'nisa', 'meama', 'wa', 'gaena', 'wisin', 'wita',\n",
        "                                'ema', 'thula', 'kota', 'yuthu', 'washayen', 'no', 'pilibadha', 'anuwa', 'eheth',\n",
        "                                'pasu', 'eak', 'mean', 'sita', 'neatha', 'weani', 'nea', 'wada', 'bohoa', 'etha',\n",
        "                                'naehea', 'thawath', 'bawata', 'pamanak', 'yam', 'samaga', 'matha', 'sae', 'namuth',\n",
        "                                'peara', 'maha', 'weatha', 'pamana', 'samaga', 'nowea', 'magin', 'enam', 'weanuwen',\n",
        "                                'yatathea', 'saema', 'pasuwa', 'thamayi', 'dhakwa', 'nowa', 'siyalu', 'yaeyi',\n",
        "                                'aethaem', 'yanu', 'waenuwen', 'haema', 'pinisa', 'patan', 'sahitha', 'hari',\n",
        "                                'aethulu', 'wagae', 'paridhi', 'pawa', 'thawa', 'samaharu', 'samahara', 'eath',\n",
        "                                'wena', 'baewin', 'pahala', 'wadaath', 'haera', 'pita', 'eweni', 'aa', 'ehala',\n",
        "                                'thulin', 'ekka', 'aadhi', 'mulu', 'sambandhayean', 'peawathi', 'oya', 'wath',\n",
        "                                'pamani', 'wen', 'pura', 'ona', 'epa', 'ewita', 'hewath', 'yanuwen', 'meawaeni',\n",
        "                                'ow', 'ethi', 'tika', 'ihatha', 'kara', 'kisiyam', 'aeyi', 'kisima', 'ara', 'heyin',\n",
        "                                'kisidhu', 'hata', 'anek', 'idhiri', 'thek', 'waenuwata', 'haetiyata', 'yata',\n",
        "                                'dhaeyi', 'mokadha', 'maghin', 'baew', 'an', 'ona', 'laga', 'uth', 'haraha',\n",
        "                                'asala', 'baehaera', 'hoth', 'ekthara', 'uda', 'athurin', 'klhi', 'ayuru', 'noyek',\n",
        "                                'atharin', 'ammea', 'aethuluwa', 'hariyatama', 'ethana', 'ehaata', 'kawara',\n",
        "                                'wanaahi', 'osseama', 'naethinam', 'udhesa', 'naedha', 'baehae', 'misa', 'haeti',\n",
        "                                'wata', 'bae', 'etharam', 'thawadhuratath', 'thawa', 'thuru', 'idhiriyae',\n",
        "                                'edhiriyae', 'hariyata', 'ebadhu', 'naethdha', 'naedhdha', 'anea', 'mona',\n",
        "                                'ethamath', 'ithamath', 'ethakota', 'aneakuth', 'awata', 'aetha', 'meabadhu',\n",
        "                                'lath', 'thulata', 'kohomadha', 'kumana', 'lesin', 'badhu', 'oonaema', 'sahagatha',\n",
        "                                'lagata', 'wethata', 'waenidhaa', 'onea', 'passea', 'thissea', 'saeti', 'baegin',\n",
        "                                'sesu',\n",
        "                                'bohoma', 'kotharam', 'aethath', 'yanuwen', 'koyi', 'upa', 'yae', 'kotharam', 'o',\n",
        "                                'anith', 'onna', 'naethahoth', 'witharak', 'kawdha', 'hugak', 'naetheyi', 'pitatha',\n",
        "                                'saema', 'nisama', 'passea', 'seaka', 'ewan', 'monawadha', 'samagama', 'thawadha',\n",
        "                                'sadha', 'haebaeyi', 'atharin', 'lesata', 'dhigata', 'mandha', 'namuth', 'namudhu',\n",
        "                                'eheanam', 'kepa', 'emagin', 'idhin', 'mokakdha', 'dhigea', 'witaka', 'meani',\n",
        "                                'sambandhawa', 'nohoth', 'anna', 'monawadha', 'harima', 'naethinam', 'atharaata',\n",
        "                                'pitathata', 'wan', 'yatin', 'wagea', 'nisath', 'pahalata', 'anya', 'eha', 'witama',\n",
        "                                'vitama', 'noyekuth', 'withara',\n",
        "                                'ihalin', 'iwatha', 'nomethiwa', 'hudeak', 'meaweni', 'waethin', 'weni', 'waewa',\n",
        "                                'andhamin', 'meanma', 'was', 'kiipa', 'kochchra', 'methaenin', 'methena',\n",
        "                                'idhiripita', 'purama', 'witaka', 'athishaya', 'dhepasa', 'peareadha', 'aethulata',\n",
        "                                'atharea', 'yamkisi', 'aassithawa', 'thak', 'koa', 'uda', 'meatha', 'ehalata',\n",
        "                                'dhenatamath', 'wenama', 'arabeaya', 'rahitha', 'pitin', 'paahea', 'aethinam',\n",
        "                                'aeththam', 'atharam', 'kuwuruth', 'paridhdhen', 'watea', 'keabadhu', 'kothek',\n",
        "                                'naththam', 'hindha', 'pasupasa', 'meha', 'ladheyi', 'paasa', 'pawa', 'perata',\n",
        "                                'waediputa', 'aethin', 'ethek', 'vina', 'maedhin', 'idhiriyean', 'pitupasa',\n",
        "                                'samaga', 'mae', 'la', 'maenawi', 'sampanna', 'athurean', 'meathram',\n",
        "                                'obbata', 'nomilea', 'arabaya', 'abiyasa', 'idhiripasa', 'enamuth',\n",
        "                                'mathin''issaraha', 'echchrara', 'ekko', 'behi', 'magin', 'magini', 'ketuwa',\n",
        "                                'misak', 'eheamyi', 'hindha', 'idhin', 'lagin', 'asalin', 'dho''kelinma',\n",
        "                                'virahitha', 'mathata', 'asalata', 'atharahura',\n",
        "                                'mealagata', 'kethek', 'kola', 'ayyo', 'nomilea', 'ethanata', 'mulata', 'lu',\n",
        "                                'samagi', 'che', 'nithi', 'wenidha', 'apoyi', 'ochchrata', 'aaubowan', 'huga',\n",
        "                                'wanathuru', 'pamanata', 'pahathata', 'ewaka', 'pasakin', 'aethulea', 'yatathata',\n",
        "                                'yabadha', 'nan',\n",
        "                                'shik', 'shek', 'usi', 'anan', 'manan', 'ananmanan', 'ahaa', 'adhdharata',\n",
        "                                'monayam', 'athana', 'aethule', 'aethulen', 'pahathin', 'virahithawa', 'aayi',\n",
        "                                'enayin', 'samagin', 'obbehi', 'othana', 'ammo', 'sha', 'neathahoth', 'atharamaga',\n",
        "                                'asabada', 'yabadhawa', 'seakwa', 'atharamaga', 'obben', 'adhdhara',\n",
        "                                'achchrara', 'dhoho', 'chi', 'chik', 'adho', 'soshae', 'ethira', 'koyibata', 'ohea',\n",
        "                                'oh', 'adhomeyi', 'uhu', 'uu', 'u', 'otharam', 'abiyasin', 'appachchiyea', 'ado',\n",
        "                                'aane', 'ane', 'hindhama', 'ha', 'hapura', 'latha', 'ohe', 'an', 'ankisi', 'hurea',\n",
        "                                'uuyi', 'adhdharin', 'embala', 'abiyasa', 'abiyasata',\n",
        "                                'uuh', 'shi', 'sisi', 'koyibata', 'satapata', 'abiyesa', 'heleyiya', 'sata',\n",
        "                                'satata'\n",
        "\n",
        "                                ]\n",
        "\n",
        "def remove_stopwords(tokenized_list):\n",
        "    text = [word for word in tokenized_list if word not in stopword]\n",
        "    #text =\" \".join( [word for word in tokenized_list if word not in stopword])\n",
        "    return text"
      ],
      "metadata": {
        "id": "v_b4uEpFOqGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stop word removal testing data\n",
        "hateSpeechData['body_text_nostop'] = hateSpeechData['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
        "hateSpeechData.head(100)"
      ],
      "metadata": {
        "id": "C7OmDzR8O_R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **F E A T U R E__E X T R A C T I O N**"
      ],
      "metadata": {
        "id": "39BZM3maUlAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vWG_D8u6UsjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **V E C T O R I Z I N G**"
      ],
      "metadata": {
        "id": "pI4KLTboUoMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-iwGAPrBUs-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}